{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO \n",
    "- [ ] Add illustrations for the concept (dinov2, instance retrieval, faiss)\n",
    "- [ ] There are a lot of possible optimizations I can apply for both search and encoding, but I don't have time for now\n",
    "- [ ] Add a pipeline to give a nice caption according to the input image and the nearest image with NLP \n",
    "- [ ] Upload it into HuggingFace space as a demo \n",
    "- [ ] Save Faiss state\n",
    "\n",
    "# References & useful resources \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/facebookresearch/dinov2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /dinov2\n",
    "# !pip install -r requirements.txt\n",
    "# !pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-gpu\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.datasets import ImageFolder\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kareem/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/kareem/mambaforge/envs/dinov2/lib/python3.9/site-packages/gradio/routes.py\", line 439, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/kareem/mambaforge/envs/dinov2/lib/python3.9/site-packages/gradio/blocks.py\", line 1384, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/kareem/mambaforge/envs/dinov2/lib/python3.9/site-packages/gradio/blocks.py\", line 1089, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/kareem/mambaforge/envs/dinov2/lib/python3.9/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/kareem/mambaforge/envs/dinov2/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/kareem/mambaforge/envs/dinov2/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/kareem/mambaforge/envs/dinov2/lib/python3.9/site-packages/gradio/utils.py\", line 700, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"<ipython-input-8-a925ade6e241>\", line 37, in return_nearest_neighbor_images\n",
      "    query_features = extract_features(query_image)\n",
      "  File \"<ipython-input-8-a925ade6e241>\", line 20, in extract_features\n",
      "    image_tensor = image_transforms(image).unsqueeze(0).to(device)\n",
      "  File \"/home/kareem/mambaforge/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/kareem/mambaforge/envs/dinov2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/kareem/mambaforge/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 361, in forward\n",
      "    return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)\n",
      "  File \"/home/kareem/mambaforge/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/functional.py\", line 476, in resize\n",
      "    _, image_height, image_width = get_dimensions(img)\n",
      "  File \"/home/kareem/mambaforge/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/functional.py\", line 78, in get_dimensions\n",
      "    return F_pil.get_dimensions(img)\n",
      "  File \"/home/kareem/mambaforge/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py\", line 31, in get_dimensions\n",
      "    raise TypeError(f\"Unexpected type {type(img)}\")\n",
      "TypeError: Unexpected type <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the DINO model\n",
    "dino = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vitb14\")\n",
    "dino.to(device)\n",
    "dino.eval()\n",
    "\n",
    "# Define the image transformations\n",
    "image_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Function to extract features from an image\n",
    "def extract_features(image):\n",
    "    image_tensor = image_transforms(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = dino(image_tensor).float()\n",
    "    return features\n",
    "\n",
    "\n",
    "# Function to perform nearest neighbor search\n",
    "def nearest_neighbor_search(query_features, dataset_features, k=5):\n",
    "    index = faiss.IndexFlatL2(query_features.shape[1])\n",
    "    dataset_features = torch.cat(dataset_features, dim=0).cpu().numpy()\n",
    "    index.add(dataset_features)\n",
    "    distances, indices = index.search(query_features.cpu().numpy(), k)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# Function to return the nearest neighbor images\n",
    "def return_nearest_neighbor_images(query_image):\n",
    "    query_features = extract_features(query_image)\n",
    "    indices = nearest_neighbor_search(query_features, dataset_features)\n",
    "    nearest_neighbor_image_paths = []\n",
    "    for i in range(k):\n",
    "        image_path = os.path.join(dataset_path, dataset_images[indices[0][i]])\n",
    "        nearest_neighbor_image_paths.append(image_path)\n",
    "    return nearest_neighbor_image_paths\n",
    "\n",
    "\n",
    "# Gradio interface\n",
    "dataset_path = \"/home/kareem/Desktop/dinv2_Image_clustering/level_up_with_dinov2/few-shot-art-painting/data\"\n",
    "dataset_images = os.listdir(dataset_path)\n",
    "dataset_features = []\n",
    "for filename in dataset_images:\n",
    "    image_path = os.path.join(dataset_path, filename)\n",
    "    if (\n",
    "        filename.endswith(\".png\")\n",
    "        or filename.endswith(\".jpg\")\n",
    "        or filename.endswith(\".jpeg\")\n",
    "    ):\n",
    "        image = Image.open(image_path)\n",
    "        features = extract_features(image)\n",
    "        dataset_features.append(features)\n",
    "\n",
    "k = 5  # Number of nearest neighbors to retrieve\n",
    "\n",
    "inputs = gr.inputs.Image(type=\"pil\", label=\"Input Image\")\n",
    "# outputs = gr.outputs.Image(type=\"pil\", label=\"Returned Images\", multiple=True)\n",
    "\n",
    "outputs = gr.Gallery()\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=return_nearest_neighbor_images,\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    title=\"Nearest Neighbor Images\",\n",
    "    description=\"Upload an image and get the nearest neighbor images from the dataset as output.\",\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
